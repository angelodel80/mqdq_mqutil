# MQDQ Migration Tool

This is a dummy tool to be used for importing MQDQ documents into Cadmus for editing, and to export them back once finished.

This tool has a CLI interface. Currently the following commands are planned:

- **partition**: partition text documents.
- **import**: import partitioned text documents, plus apparatus where available, into a Cadmus database.
- **export**: regenerate text files and apparatus files from the Cadmus database. The TEI headers, which are not imported in Cadmus, will be merged with the output text documents.

Note for users: if not using a development machine (which has the SDK installed), you must install the [.NET Core runtime](https://dotnet.microsoft.com/download/dotnet-core) to run this program.

## Partitioning

### Rationale

As we are dealing with text, we must define a partitioning criterion. We should examine the documents typical structure(s) and metrics to define the best strategy.

Of course, in the context of such editing system, we could not think of storing the whole text in a single unit, like in a big text file; this would not be scalable, hamper the highly networked nature of the parts, and make impossible to concurrently edit different parts of it. We must then partition our documents.

This partitioning must be driven by the intrinsic structure of the text, so that each resulting text part represents a meaningful, self-contained unit. For instance, in a corpus of inscriptions the item would be an inscription; in a corpus of epigrams, it would be an epigram; in a prose corpus of Platonic dialogs, it would be a paragraph. As for their typographical nature, paragraphs are modern units determined by the traditional text layout; but of course they obey the text meaning, by grouping one or more complete sentences into a relatively self-contained unit.

Of course this is an arbitrary choice; but, in a sense, this type of text partitioning is no different from the divisions applied to the structure of a TEI document. For instance, imagine a Plato's dialog where each part is a paragraph, and a corresponding TEI text where each paragraph is marked by p: as for archiving, the substantial difference would be that each paragraph gets separately stored in the database, while it is usually contained in a single file in XML. Joining text parts to rebuild the unique flow of base text would of course be trivial, whether we are exporting data to generate TEI, or just displaying a continuous text.

Naturally, such decomposition, functional to the software requirements and to the high density of metatextual data with their multiple connections, would be an overkill if applied outside the scenarios which practically defined the birth of the system itself.

The partition function targets MQDQ XML document comments. Its primary output will be a copy of the original document, with some `pb` elements inserted.

This element will have a `@n` attribute with the full citation (see below) of the partition ended by the `pb` element itself. This way, users will be able to check the process before we apply it.

This element is chosen because `pb` is never used in the original documents, and we need an empty element to avoid breaking the existing XML structure.

An importer will then just have to look at these `pb` elements, and store an item for each partition, with a text part and its citation. The citation will be provided in the item's description, and the item's sort key will be generated by joining the file name with the line ID.

Note that further processing will be required for `l`'s text, as it may contain non-textual data in the form of text, like e.g. the annotations used to aid metrics (see below).

- if there is `div2`, partition = `div2`.
- else: look at `div1@type` value:
  - `section`, `fragment`, `fragments`: e.g. a book in the Aeneid, or a poem in Catullus, or a fragment: partition = `div1`. For value section, the `div1` might be too long (e.g. a book in the Aeneid). In this case, refer to the algorithm below ('too long' here means >M).
  - work: a full work without further divisions: partition = `div1`. If >M, apply partitioning.

For partitioning case `div1@type=section` when this is too long (e.g. >50 lines), we must use an algorithmic approach, following these principles:

- min lines count treshold = N;
- max lines count treshold = M;
- break after the first `l` whose content matches the regex `[\u037e.?!][^\p{L}]*$` (=stop/exclamation/question mark at line end);
- if no match, prefer the largest partition (if any) below N, or just force a break at M;
- in the corner case where the starting `l` of the next partition is the last child of `div1`, this will be joined to the previous partition.

Also, the partitioning process requires us to calculate a **citation** for each partition. In general, the citation tells us exactly the portion of the source document which was cut, e.g. 3,12-36 for lines 12-36 in book 3 of Vergilius' Aeneid. In our case, we need to be able to rebuild the XML from the database, so we must keep all the metadata linked to the text division the partition belongs to. This won't be a nice citation, but it will serve its legacy-compliant purpose.

The citation is built by concatenating these components, separated by space:

1. the file name without extension (e.g. `LVCR-rena`).
2. `div1` attributes, each with the form name=value separated by pipes (e.g. `xml:id=d001|type=section|decls=#md|met=H`). This assumes that no attribute value contains a pipe.
3. `div2` attributes, when there is a `div2`.
4. the line number is always found at `l@n`.
5. `l@xml:id` must be preserved, too; we append it to the line number after a `#` character.

Thus, a citation would be like this: `LVCR-rena xml:id=d001|type=section|decls=#md|met=H 12#00122`.

In theory, this should ensure that we can rebuild the whole XML text body from the partitioned texts, provided that:

- we can algorithmically decide whether we must emit `div1`/`div2` according to the `div1` attributes.
- `div1` and `div2` models are never mixed content (i.e. there is no meaningful text between their tags).
- `l` elements do not have children elements, but only contain text, except the case of documents with apparatus. In this case, each single word is wrapped in a `w` element.
- portions from the same file are sorted according to their line IDs (portion after `#` in the sample above). Note that I'm taking the ID as reference, in the assumption that (a) IDs were assigned sequentially; and (b) numbers might be out-of-order (e.g. a line 12 moved between line 16 and 17).

### Partition Command

Partition all the files matching the specified mask and requiring partitioning, saving a copy of each partitioned file in the specified output directory.

The output files will be equal to the input files, except for the addition of `pb` elements at the end of each partition.

Syntax:

```ps1
Mqutil partition <InputFilesMask> <OutputDir> [-n MIN] [-m MAX]
```

where:

- `InputFilesMask` is the input file(s) mask.
- `OutputDir` is the output directory (will be created if not exists).
- `-n` is the optional minimum treshold (default 20).
- `-m` is the optional maximum treshold (default 50).

Just launch the program without arguments to get help directions. This gets a generic help, which also tells you how to get help about any specific command.

Note: for Linux users, you should run the program like this:

```bash
dotnet ./Mqutil.dll ...arguments...
```

## Modeling

At a minimum, these should be the MQDQ parts:

- **text part**: the base text. Each text part structures its text into a set of lines, and has a citation, a free string representing the location of the text in the work.
- **witnesses part**: witnesses quoting the text in the context of their specific work. This should be a list, where each witness has a `citation` identifying it in some citational conventions system (e.g. `Fest. p.376`), and a free Markdown `text` with the context of the citation.

Layer parts:

- **apparatus** layer part: critical apparatus. See below.
- **comment** layer part: comments linked to a specific portion of the base text. Each comment fragment has a tag which can be used for classification, and a free Markdown text.
- **orthography-patch** part: orthographic normalization to aid metrical analysis e.g. `uoluisti` for `voluisti` Lucr. 1,32. In the original text, this appears as text appended to the word between `(==` and `)`; in the model, it is represented by a `patch` property. This is not the same as the orthography layer proper, which is a specialized layer targeted to editorial and linguistic purposes (e.g. `bixit`=`vixit`). The patch layer is just a layer containing some practical patches to the original orthography, as far as they are required by the metrical analysis system.
- **word-anchors** part: for documents with apparatus, where each word is wrapped inside a `w` element. We just use this to map between the Cadmus coordinates system for that part, and the ID assigned to each word in the original document, as these are the anchors to which apparatus entries get attached. I assume that "word" here is just what is separated by whitespace: this seems to be the case also for MQDQ. Thus, each fragment just has an `id` property.

The last 2 parts are specific to legacy compatibility requirements for MQDQ, and will not be edited at all (assuming of course that we're not changing the text itself); they just retain data to be used for export. All the others parts instead are generic.

### Apparatus

Each apparatus layer fragment has these properties (besides `location`, which is common to all the fragments):

- `lemmaVariantType` (enum): one of `replacement`, `additionBefore`, `additionAfter`, `note`. In fact, each fragment in the apparatus is either a replacement, an addition, or just a generic note related to the constitution of the text (e.g. "dubitat Crusius an interpungendum sit").
- `value` (string): the entry value is either a text variant (zero for a deletion), or the value of the text note, according to the fragment's `lemmaVariantType`.
- `isAccepted` (boolean): true if the variant has been accepted, so tat it should replace the lemma in the text.
- `authors` (string array): list of authors (usually these are short IDs to be resolved somewhere outside the fragment).
- `note`: optional Markdown annotation.

## Importing

**NOTE**: this is just a plan, development will follow.

Importing requires that all the documents requiring partitioning have been partitioned. The others instead can be manually copied in the import directory as they are.

The general procedure for importing could be as follows:

1. open the XML text document. If it contains any `pb` element, it's a partitioned document; else, it's an unpartitioned document (=a document which did not require partitioning).

2. determine the partitions boundaries:

- for unpartitioned documents, each partition is either `div2` (when any `div2` is present), or `div1` (when no `div2` is present), as a whole.
- for partitioned documents, each partition is the all the children elements of each `div1` (with all of their descendants), up to the first `pb` child, or up to the `div1` end.

3. determine the partitions citations:

- for partitions closed by `pb`, each `pb@n` attribute contains the citation.
- else, each partition must build its citation from the `div2`/`div1` parent element, just like the citation built by the partitioner (see above).

4. determine the presence of metatextual characters in the text, i.e. the occurrence of text appended to the word between `(==` and `)`. In this case:

- the metadata characters must be removed from the text.
- the content of `(==...)` become the `patch` property of an orthographic patches layer. Its coordinates are the line ordinal number (y) and the token ordinal number (x).

The outcome of these operations is:

- 1 item per partition; its title will be equal to the concatenation of these portions of the partition citation: file name, space, and `l`'s `id`. For instance, `LVCR-rena 00122` from `LVCR-rena xml:id=d001|type=section|decls=#md|met=H 12#00122`. This should allow sorting the items in their natural order, by just sorting them by title (which is what is done by the standard item sort key generator in Cadmus, apart from normalizations);
- 1 text part per item;
- 0-1 orthographic patches layer part per item.

TODO: apparatus. This must be retrieved from files having it, and mapped to parts.
